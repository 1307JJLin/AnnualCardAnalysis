{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022年度数据分析报告 ｜杰然不同之GR\n",
    "\n",
    "\n",
    "# 本报告由公众号【杰然不同之GR】编写，整理并发布，仅作个人学习使用，请勿用于任何商业用途，转载及其他形式合作请与我联系 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简易目录\n",
    "\n",
    "1.数据预处理\n",
    "   \n",
    "1.1 数据合并和一些纠错\n",
    "\n",
    "1.2 新特征生成\n",
    "\n",
    "1.2.1) 根据date生成月日还有星期\n",
    "\n",
    "1.2.2) 根据start time 和end time 生成时和分\n",
    "\n",
    "1.2.3) 根据start和end生成中间时间\n",
    "\n",
    "1.2.4) 根据事项的中间时间确定事项做的时候处于一天的时段\n",
    "\n",
    "1.2.5) 根据时长大小分为长中短\n",
    "\n",
    "1.2.6) 修改一些前后差别不大的事件\n",
    "\n",
    "1.2.7) 统一事项与属性的对应关系 \n",
    "\n",
    "2.数据可视化\n",
    "\n",
    "2.1 变量分类\n",
    "\n",
    "2.1.1) 日期类变量: date\n",
    "\n",
    "2.1.2) 数值类变量: duration, phone \n",
    "\n",
    "2.1.3) 类别变量: event, year, month, day, weekday, mid_hour, day_period, week_order, duration_attr, attr, \n",
    "\n",
    "2.2 单变量分析\n",
    "\n",
    "2.2.1) 数值变量的直方图\n",
    "\n",
    "2.2.2) 类别变量的饼图，柱形图\n",
    "\n",
    "2.2.3) 专注力分析\n",
    "\n",
    "2.2.4) 各个月份，各年, 各小时的event关键词词云图\n",
    "\n",
    "2.3 双变量分析\n",
    "\n",
    "2.3.1) PyCatFlow图\n",
    "\n",
    "2.3.2) 单个类别变量 vs 数值 sns.stripplot  sns.swarmplot  sns.boxplot   sns.violinplot  \n",
    "\n",
    "2.3.3) 两个类别变量 VS 数值 sns.countplot  sns.barplot  sns.factorplot  sns.pointplot \n",
    "\n",
    "2.4 衍生数据可视化\n",
    "\n",
    "2.4.1) 日度汇总各类时间总时长\n",
    "\n",
    "2.4.2) 每天的第一件和最后一件事\n",
    "\n",
    "2.4.3) 每天的第一件和最后一件事的时间\n",
    "\n",
    "2.4.4) 每天做的事项数目的统计\n",
    "\n",
    "2.4.5) 每天做事项先后编号并统计\n",
    "\n",
    "2.4.6) 各事项关于日期求和的透视表\n",
    "\n",
    "2.4.7) 每年, 月, 星期的事项关键词词云图\n",
    "\n",
    "2.4.8) 每天事项的对应先后以及月度关联的网络图\n",
    "\n",
    "2.5 一键式EDA\n",
    "\n",
    "3.数据分析\n",
    "\n",
    "3.1 方差分析每月或是每年的事项是否有差异\n",
    "\n",
    "3.1.1) 原始事项时长数据的方差分析\n",
    "\n",
    "3.1.2) 衍生数据的方差分析\n",
    "\n",
    "3.2 LSTM预测前后事项\n",
    "\n",
    "3.2.1) 建立字符索引\n",
    "\n",
    "3.2.2) 建立LSTM模型\n",
    "\n",
    "3.2.3) 模型训练\n",
    "\n",
    "3.3 时长数据的简单聚类\n",
    "\n",
    "3.4 关于日期的缺失值处理\n",
    "\n",
    "3.5 关于日期的傅里叶分解求周期\n",
    "\n",
    "3.6 关于日期的传统时间序列分析\n",
    "\n",
    "3.6.1) ARIMA分析\n",
    "\n",
    "3.6.2) GARCH分析\n",
    "\n",
    "3.6.3) VAR分析\n",
    "\n",
    "3.7 关于日期的NeuralProphet 时间序列\n",
    "\n",
    "4.数据挖掘\n",
    "\n",
    "4.1 通过生成聚合特征做日度时长数据的特征工程\n",
    "\n",
    "4.2 类别变量的one-hot encoding\n",
    "\n",
    "4.3 利用boruta筛选特征\n",
    "\n",
    "4.4 利用PCA降维\n",
    "\n",
    "4.5 利用optuna优化参数\n",
    "\n",
    "4.6 stacking 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import jieba\n",
    "import statsmodels.tsa.stattools as st\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import networkx as nx \n",
    "import pycatflow as pcf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "from collections import Counter\n",
    "import optuna\n",
    "import pylab as pl \n",
    "import matplotlib.cm as cm \n",
    "import torch\n",
    "import random\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.stats import kstest, shapiro\n",
    "from optuna.samplers import TPESampler\n",
    "from gensim.corpora import Dictionary \n",
    "from gensim.models import TfidfModel\n",
    "from wordcloud import WordCloud\n",
    "from scipy import stats, signal, interpolate\n",
    "from arch import arch_model\n",
    "from boruta import BorutaPy\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa import stattools\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "from tqdm import tqdm\n",
    "from neuralprophet import NeuralProphet\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib import rcParams\n",
    "from torch import nn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from utils import part1, part2, part3, part4, funcs\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "config = {\n",
    "    \"font.family\":'serif',\n",
    "    \"font.size\": 40,\n",
    "    'font.weight':'bold',\n",
    "    \"mathtext.fontset\":'stix',\n",
    "    'figure.titleweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    \"font.serif\": ['STSong'],\n",
    "    'axes.unicode_minus':False\n",
    "}\n",
    "rcParams.update(config)\n",
    "path = 'D:/打卡/2022/年度总结'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据合并和一些纠错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环读入每天的数据\n",
    "df = pd.DataFrame()\n",
    "for month in [f'0{n}' for n in range(1, 10)] + ['10', '11', '12']:\n",
    "    for day in [f'0{n}' for n in range(1, 10)] + [str(n) for n in range(10, 32)]:\n",
    "        file = f'2022-{month}-{day}打卡记录.xlsx'\n",
    "        phone = f'2022-{month}-{day}打卡总结.xlsx'\n",
    "        try:\n",
    "            temp = pd.read_excel(file)\n",
    "            temp['phone'] = pd.read_excel(phone).loc[0, '手机时间']\n",
    "            df = pd.concat([df, temp], axis=0)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些纠错\n",
    "corrects = {\n",
    "    '口音':'口语', \n",
    "    '策划摄影':'摄影',\n",
    "    '买食物':'购物', \n",
    "    '驾校体检':'咨询驾校', \n",
    "    '洗澡':'洗漱',\n",
    "    '法律咨询':'咨询',\n",
    "    '生日准备':'购物',\n",
    "    '买蛋糕':'购物',\n",
    "    '蒸饭':'煮饭',\n",
    "    '准备礼物':'购物',\n",
    "    '摄影策划':'摄影',\n",
    "    '拿外卖':'拿快递',\n",
    "    '买东西':'购物'\n",
    "}\n",
    "\n",
    "df['event'] = df['event'].apply(lambda x: corrects.get(x) if corrects.get(x) is not None else x)\n",
    "df.to_excel('2022汇总.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  新特征生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_excel(f'{n}汇总.xlsx')['date\tevent\tstart\tend\tduration\tattr\tphone'.split()]\n",
    "        for n in [2019, 2020, 2021, 2022]\n",
    "    ], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 根据date生成月日还有星期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.1 根据date生成月日还有星期\n",
    "df['event'] = df['event'].str.strip()\n",
    "df['year'] = df['date'].astype(str).apply(lambda x:str(x.split('-')[0]))\n",
    "df['month'] = df['date'].astype(str).apply(lambda x:str(x.split('-')[1]))\n",
    "df['day'] = df['date'].astype(str).apply(lambda x:str(x.split('-')[2]))\n",
    "df['weekday'] = pd.to_datetime(df['date']).dt.weekday + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 根据start time和end time生成时和分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.2 根据start time和end time生成时和分\n",
    "df['start_hour'] = df['start'].astype(str).apply(lambda x:str(x.split(':')[0]))\n",
    "df['end_hour'] = df['end'].astype(str).apply(lambda x:str(x.split(':')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 根据start和end生成中间时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.3 根据start和end生成中间时间\n",
    "df['mid_time'] = df.apply(lambda x: part1.get_mid(x['start'], x['duration']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 根据事项的中间时间确定事项做的时候处于一天的时段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.4 根据事项的中间时间确定事项做的时候处于一天的时段\n",
    "df['mid_hour'] = df['mid_time'].astype(str).apply(lambda x:str(x.split(':')[0]))\n",
    "df['day_period'] = df['mid_hour'].map(part1.get_period)\n",
    "df['week_order'] = df['date'].map(lambda x: pd.Period(x, freq='W'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 根据时长大小分为长中短"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration_attr'] = df['duration'].map(part1.get_duration)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 修改一些前后差别不大的事件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.6 修改一些前后差别不大的事件\n",
    "df['event'] = df['event'].map(part1.correct_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 事项与属性的对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2.7 事项与属性的对应\n",
    "# 各别事项前后不统一 需要统一修正 修正成该事项出现最多的属性\n",
    "events = df['event'].unique()\n",
    "event2attr = {}\n",
    "for e in events:\n",
    "    attr = df[df['event'] == e]['attr'].value_counts().index[0]\n",
    "    event2attr[e] = attr\n",
    "\n",
    "df['attr'] = df['event'].map(event2attr)\n",
    "df.to_excel('4年汇总.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否还存在事项的属性不唯一的情况\n",
    "df[['event', 'attr']].drop_duplicates().to_excel('事项性质对应表.xlsx', index=False)\n",
    "df[['event', 'attr']].drop_duplicates()['event'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 变量分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_vars = ['date']\n",
    "years = [2022]#range(2019, 2023)\n",
    "\n",
    "numeric_vars = [\n",
    "    'duration', 'phone'\n",
    "]\n",
    "\n",
    "cate_vars = [\n",
    "    'event', 'year', 'month', 'day', \n",
    "    'weekday', 'mid_hour', 'day_period', 'week_order', 'duration_attr', 'attr'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 单变量分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) 数值变量的直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1) 数值变量的直方图\n",
    "part = '2.2.1'\n",
    "for col in numeric_vars:\n",
    "    for year in years:\n",
    "        part2.numeric_hist(df, col, year, part, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) 类别变量的饼图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2) 类别变量的饼图\n",
    "# 类别变量饼图\n",
    "part = '2.2.2'\n",
    "\n",
    "for col in cate_vars:\n",
    "    if col == 'year':\n",
    "        continue\n",
    "    for year in years:\n",
    "        part2.plot_pie(df, col, 15, part, path, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3) 专注力分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.3) 专注力分析\n",
    "part = '2.2.3'\n",
    "title = f'{part} 有效时间在不同年和时长段的柱形图'\n",
    "temp = df[df['attr'] == '有效时间'].groupby(['year', 'duration_attr'])['date'].\\\n",
    "    count().unstack()\n",
    "temp = temp / temp.sum(axis=0)  #计算频率\n",
    "temp.plot(\n",
    "        kind='bar', \n",
    "        figsize=(20, 10), \n",
    "    )\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Year', fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.concentrate(df, 2022, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.concentrate(df, 2021, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.concentrate(df, 2020, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.concentrate(df, 2019, part, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 双变量分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1) PyCatFlow图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.1) PyCatFlow图\n",
    "column_order = { \n",
    "    date: i + 1 for i, date in enumerate(df['date'].unique()) \n",
    "}\n",
    "df['column_order'] = df['date'].map(column_order)\n",
    "translate = pd.read_csv('event.csv', encoding='utf-8')\n",
    "chi = translate['Chinese'].values\n",
    "eng = translate['English'].dropna().values\n",
    "translates = dict(zip(chi, eng))\n",
    "\n",
    "translates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eng_event'] = df['event'].map(translates)\n",
    "\n",
    "df['eng_attr'] = df['attr'].map({\n",
    "    '有效时间':'effective',\n",
    "    '浪费时间':'wasteful',\n",
    "    '必要时间':'necessary',\n",
    "})\n",
    "\n",
    "temp = df[df['year'] == '2022']\n",
    "temp[['eng_event', 'eng_attr', 'date', 'column_order']].\\\n",
    "to_csv('2022.csv', encoding='gbk', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and parsing data:\n",
    "data = pcf.read_file(\n",
    "    \"2022.csv\", \n",
    "    columns=\"date\", \n",
    "    nodes=\"eng_event\", \n",
    "    categories=\"eng_attr\", \n",
    "    column_order=\"column_order\"\n",
    ")\n",
    "\n",
    "# Generating the visualization\n",
    "viz = pcf.visualize(\n",
    "    data,\n",
    "    spacing=100,\n",
    "    width=88000,\n",
    "    maxValue=20,\n",
    "    minValue=2\n",
    ")\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2) 单个类别变量 vs 数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.2) 单个类别变量 vs 数值\n",
    "# 类别变量 VS 数值变量\n",
    "part = '2.3.2'\n",
    "for num in numeric_vars:\n",
    "    for cate in cate_vars:\n",
    "        part2.plot_num_vs_cate(df, num, cate, path, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in numeric_vars:\n",
    "    for cate in cate_vars:\n",
    "        part2.plot_box(df, num, cate, path, part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3) 两个类别变量 VS 数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3.3) 两个类别变量 VS 数值\n",
    "part = '2.3.3'\n",
    "for idx, v in enumerate(cate_vars):\n",
    "    if idx == len(cate_vars) - 1:\n",
    "        break\n",
    "    for k in cate_vars[idx + 1:]:\n",
    "        part2.plot_count_plot(df, v, k, path, part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 衍生数据可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1) 日度汇总各类时间总时长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1) 日度汇总各类时间总时长\n",
    "part = '2.4.1'\n",
    "daily_attr = df.groupby(['date', 'attr'])['duration'].\\\n",
    "    agg('sum').unstack().reset_index()\n",
    "\n",
    "daily_attr = pd.merge(\n",
    "    daily_attr, \n",
    "    df[['date', 'phone']].drop_duplicates(), \n",
    "    on='date'\n",
    ").rename(columns={'phone':'手机时间'})\n",
    "\n",
    "daily_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_attr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_attr[daily_attr['date'] >= '2022-01-01'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 日度汇总各类时间总时长'\n",
    "daily_attr.set_index('date').plot(\n",
    "    kind='line', figsize=(20, 10)\n",
    ")\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Sum of daily duration', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(loc=0, fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成新数据\n",
    "daily_attr['weekday'] = pd.to_datetime(daily_attr['date']).dt.weekday + 1\n",
    "daily_attr['month'] = daily_attr['date'].apply(lambda x:str(x).split('-')[1])\n",
    "daily_attr['day'] = daily_attr['date'].apply(lambda x:str(x).split('-')[2])\n",
    "daily_attr['week_order'] = daily_attr['date'].map(lambda x: pd.Period(x, freq='W'))\n",
    "daily_attr['day_of_year'] = pd.to_datetime(daily_attr['date']).dt.day_of_year\n",
    "daily_attr['year'] = pd.to_datetime(daily_attr['date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每年各类时间折线图对比\n",
    "for col in '必要时间\t有效时间\t浪费时间\t手机时间'.split():\n",
    "    part2.line_compare(daily_attr, col, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插值\n",
    "daily_attr_copy = daily_attr.copy()\n",
    "daily_attr_copy = daily_attr_copy.rename(\n",
    "    columns={\n",
    "        '必要时间': 'necessary time',\t\n",
    "        '有效时间': 'effective time',\t\n",
    "        '浪费时间': 'waste time',\t\n",
    "        '手机时间': 'phone time',\n",
    "    }\n",
    ")\n",
    "daily_attr_copy = daily_attr_copy.dropna()\n",
    "daily_attr_copy = daily_attr_copy[daily_attr_copy['date'] >= '2022-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = 'waste time', 'necessary time', 'effective time'\n",
    "part2.daily_interpolate(daily_attr_copy, x, y, z, path, part, n=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = 'phone time', 'necessary time', 'effective time'\n",
    "part2.daily_interpolate(daily_attr_copy, x, y, z, path, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4年各类时间在月，日，星期的对比\n",
    "for c in ['weekday', 'day_of_year', 'month']:\n",
    "    for tt in '必要时间\t有效时间\t浪费时间\t手机时间'.split():\n",
    "        for fun in [part2.get_max_sum_year, part2.get_max_mean_year]:\n",
    "            part2.plot_max_year(daily_attr, c, tt, fun, part, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2) 每天的第一件和最后一件事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2) 每天的第一件和最后一件事\n",
    "part = '2.4.2'\n",
    "first_last_event = pd.DataFrame()\n",
    "\n",
    "# 每天做的第一件事\n",
    "first_last_event['first_event'] = df[['date', 'event']].\\\n",
    "    groupby(['date'])['event'].first()\n",
    "\n",
    "# 每天做的最后一件事\n",
    "first_last_event['last_event'] = df[['date', 'event']].\\\n",
    "    groupby(['date'])['event'].last()\n",
    "\n",
    "    \n",
    "first_last_event = first_last_event.reset_index()\n",
    "first_last_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['year'])['event'].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['year', 'event'])['duration'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整体的饼图\n",
    "part2.plot_pie(first_last_event, 'first_event', 15, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019饼图\n",
    "part2.plot_pie(\n",
    "    first_last_event[first_last_event['date'] < '2020-01-01'], \n",
    "    'first_event', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2019\n",
    ")\n",
    "part2.plot_pie(\n",
    "    first_last_event[first_last_event['date'] < '2020-01-01'], \n",
    "    'last_event', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2019\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020饼图\n",
    "temp = first_last_event[\n",
    "    (first_last_event['date'] < '2021-01-01') & (first_last_event['date'] >= '2020-01-01')\n",
    "]\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'first_event', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2020\n",
    ")\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'last_event', \n",
    "    5, \n",
    "    part, \n",
    "    path,\n",
    "    year=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021饼图\n",
    "temp = first_last_event[\n",
    "    (first_last_event['date'] < '2022-01-01') & (first_last_event['date'] >= '2021-01-01')\n",
    "]\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'first_event', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2021\n",
    ")\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'last_event', \n",
    "    5, \n",
    "    part, \n",
    "    path,\n",
    "    year=2021\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022饼图\n",
    "temp = first_last_event[\n",
    "    (first_last_event['date'] < '2023-01-01') & (first_last_event['date'] >= '2022-01-01')\n",
    "]\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'first_event', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2022\n",
    ")\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'last_event', \n",
    "    5, \n",
    "    part, \n",
    "    path,\n",
    "    year=2022\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3) 每天的第一件和最后一件事的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.3) 每天的第一件和最后一件事的时间\n",
    "part = '2.4.3'\n",
    "first_last_time = pd.merge(\n",
    "    df.groupby(['date'])['start', 'start_hour'].first(), \n",
    "    df.groupby(['date'])['end', 'end_hour'].last(), \n",
    "    on='date'\n",
    ").reset_index().rename(columns={\n",
    "    'start':'first_start_time', \n",
    "    'start_hour':'first_start_hour',\n",
    "    'end':'last_end_time', \n",
    "    'end_hour':'last_end_hour'\n",
    "})\n",
    "\n",
    "first_last_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019饼图\n",
    "temp = first_last_time[first_last_time['date'] < '2020-01-01']\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'first_start_hour', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2019\n",
    ")\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'last_end_hour', \n",
    "    5, \n",
    "    part, \n",
    "    path,\n",
    "    year=2019\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020饼图\n",
    "temp = first_last_time[\n",
    "    (first_last_time['date'] < '2021-01-01') & (first_last_time['date'] >= '2020-01-01')\n",
    "]\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'first_start_hour', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2020\n",
    ")\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'last_end_hour', \n",
    "    5, \n",
    "    part, \n",
    "    path,\n",
    "    year=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021饼图\n",
    "temp = first_last_time[\n",
    "    (first_last_time['date'] < '2022-01-01') & (first_last_time['date'] >= '2021-01-01')\n",
    "]\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'first_start_hour', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2021\n",
    ")\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'last_end_hour', \n",
    "    5, \n",
    "    part, \n",
    "    path,\n",
    "    year=2021\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022饼图\n",
    "temp = first_last_time[\n",
    "    (first_last_time['date'] < '2023-01-01') & (first_last_time['date'] >= '2022-01-01')\n",
    "]\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'first_start_hour', \n",
    "    15, \n",
    "    part, \n",
    "    path,\n",
    "    year=2022\n",
    ")\n",
    "part2.plot_pie(\n",
    "    temp, \n",
    "    'last_end_hour', \n",
    "    5, \n",
    "    part, \n",
    "    path,\n",
    "    year=2022\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4) 每天做的事项数目的统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.4) 每天做的事项数目的统计\n",
    "part = '2.4.4'\n",
    "df.groupby('date')['event'].count().\\\n",
    "    reset_index().\\\n",
    "    sort_values(by='event', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每天做的事项数目的统计'\n",
    "df.groupby('date')['event'].count().plot(\n",
    "    figsize=(20, 10)\n",
    ")\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Count', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每天做的事项数目的统计直方图'\n",
    "df.groupby('date')['event'].count().plot(\n",
    "    kind='hist', figsize=(20, 10)\n",
    ")\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Count', fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5) 各年，每天，每月做得最多的事以及各年的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各年\n",
    "df.groupby(['year'])[['event']].agg(\n",
    "    lambda x:','.join(x[0] for x in Counter(x).most_common(10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每月做的次数最多的事\n",
    "df.groupby(['month'])[['event']].agg(\n",
    "    lambda x:','.join(x[0] for x in Counter(x).most_common(10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各天做的次数最多的事\n",
    "df.groupby(['weekday'])[['event']].agg(\n",
    "    lambda x:','.join(x[0] for x in Counter(x).most_common(10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各年做的总时间最多的事\n",
    "df.groupby(['year', 'event'])[['duration']].sum().\\\n",
    "    reset_index().groupby('year')[['event', 'duration']].apply(\n",
    "        lambda x:x[x['duration'].isin(x['duration'].nlargest(3))].\\\n",
    "            sort_values(by='duration', ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各月做的总时间最多的事\n",
    "df.groupby(['month', 'event'])[['duration']].sum().\\\n",
    "    reset_index().groupby('month')[['event', 'duration']].apply(\n",
    "        lambda x:x[x['duration'].isin(x['duration'].nlargest(5))].\\\n",
    "            sort_values(by='duration', ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各天做的总时间最多的事\n",
    "df.groupby(['weekday', 'event'])[['duration']].sum().\\\n",
    "    reset_index().groupby('weekday')[['event', 'duration']].apply(\n",
    "        lambda x:x[x['duration'].isin(x['duration'].nlargest(5))].\\\n",
    "            sort_values(by='duration', ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各年的余弦相似度\n",
    "for col in ['year', 'month', 'weekday']:\n",
    "    part2.plot_sim_heatmap(df, col, part, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6) 每天做事项先后编号并统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.6) 每天做事项先后编号并统计\n",
    "part = '2.4.6'\n",
    "def get_order(alist):\n",
    "    return range(1, 1 + len(alist))\n",
    "\n",
    "daily_event_order = df.groupby('date')['event'].agg(get_order)\n",
    "daily_event_order = pd.concat(\n",
    "    [daily_event_order.explode().reset_index().rename(columns={'event':'order'}), \n",
    "    df['event'].reset_index()], axis=1\n",
    ")[['date', 'event', 'order']]\n",
    "daily_event_order['order'] = daily_event_order['order'].astype(int)\n",
    "daily_event_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个事项平均处于每天的第几件事\n",
    "event_mean_order = daily_event_order[\n",
    "    (daily_event_order['date'] < '2023-01-01') & \n",
    "    (daily_event_order['date'] >= '2022-01-01')\n",
    "].groupby('event')['order'].mean()\n",
    "\n",
    "title = f'{part} 各事项平均顺序的统计柱形图'\n",
    "event_mean_order.sort_values().plot(\n",
    "    kind='bar', figsize=(20, 10)\n",
    ")\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Event', fontsize=20)\n",
    "plt.ylabel('Mean Order', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个次序做得最多的事\n",
    "order_most_event = daily_event_order[\n",
    "    (daily_event_order['date'] < '2023-01-01') & \n",
    "    (daily_event_order['date'] >= '2022-01-01')\n",
    "].groupby(['order', 'event'])['date'].agg('count').reset_index()\n",
    "\n",
    "order_most_event = order_most_event.rename(\n",
    "    columns={'date': 'count'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个顺序做的最多的k件事\n",
    "k = 1\n",
    "order_mostk_event = pd.concat(\n",
    "    [order_most_event[order_most_event['order'] == m].\\\n",
    "    sort_values(by='count', ascending=False)[:k] \n",
    "    for m in order_most_event['order'].unique()], axis=0\n",
    ")\n",
    "order_mostk_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.7) 各事项关于日期的透视表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.7) 各事项关于日期的透视表\n",
    "part = '2.4.7'\n",
    "\n",
    "# 日度时长总和透视\n",
    "sum_pivot = part2.get_pivot(df, 'date', 'event', 'duration', 'sum')\n",
    "sum_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接单工作每天总时长分析\n",
    "print(\n",
    "    'The mean time for 接单工作 in year 2022 every day is', sum_pivot['event_duration_pivot_sum_接单工作'][\n",
    "                        sum_pivot.index >= '2022-01-01'\n",
    "].mean()\n",
    ")\n",
    "\n",
    "sum_pivot['event_duration_pivot_sum_接单工作'][\n",
    "    sum_pivot['event_duration_pivot_sum_接单工作'] > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每日接单工作时长折线图'\n",
    "\n",
    "sum_pivot['event_duration_pivot_sum_接单工作'][\n",
    "    sum_pivot['event_duration_pivot_sum_接单工作'] > 0\n",
    "].plot(figsize=(20, 10), title=title)\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Sum of duration', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每日接单工作时长直方图'\n",
    "\n",
    "sum_pivot['event_duration_pivot_sum_接单工作'][\n",
    "    sum_pivot['event_duration_pivot_sum_接单工作'] > 0\n",
    "].plot(\n",
    "    kind='hist', figsize=(20, 10), title=title\n",
    ")\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Sum of duration', fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 磨蹭每天总时长分析\n",
    "print(\n",
    "    'The mean time for 磨蹭 in year 2022 every day is', sum_pivot['event_duration_pivot_sum_磨蹭'][\n",
    "                    sum_pivot.index >= '2022-01-01'\n",
    "].mean()\n",
    ")\n",
    "\n",
    "sum_pivot['event_duration_pivot_sum_磨蹭'][\n",
    "    sum_pivot['event_duration_pivot_sum_磨蹭'] > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每日磨蹭时长折线图'\n",
    "\n",
    "sum_pivot['event_duration_pivot_sum_磨蹭'][\n",
    "    sum_pivot['event_duration_pivot_sum_磨蹭'] > 0\n",
    "].plot(figsize=(20, 10), title=title)\n",
    "\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Sum of duration', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每日磨蹭时长直方图'\n",
    "\n",
    "sum_pivot['event_duration_pivot_sum_磨蹭'][\n",
    "    sum_pivot['event_duration_pivot_sum_磨蹭'] > 0\n",
    "].plot(\n",
    "    kind='hist', figsize=(20, 10), title=title\n",
    ")\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Count', fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次数透视\n",
    "count_pivot = part2.get_pivot(df, 'date', 'event', 'duration', 'count')\n",
    "count_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接单工作次数分析\n",
    "title = f'{part} 每日接单工作次数折线图'\n",
    "\n",
    "count_pivot['event_duration_pivot_count_接单工作'][\n",
    "    count_pivot['event_duration_pivot_count_接单工作'] > 0\n",
    "].plot(figsize=(20, 10), title=title)\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Count', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每日接单工作次数直方图'\n",
    "\n",
    "count_pivot['event_duration_pivot_count_接单工作'][\n",
    "    count_pivot['event_duration_pivot_count_接单工作'] > 0\n",
    "].plot(kind='hist', figsize=(20, 10), title=title)\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Count', fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 磨蹭次数分析\n",
    "print(\n",
    "    'The mean count for 磨蹭 in year 2022 every day is', count_pivot['event_duration_pivot_count_磨蹭'][\n",
    "                    count_pivot.index >= '2022-01-01'\n",
    "].mean()\n",
    ")\n",
    "\n",
    "count_pivot['event_duration_pivot_count_磨蹭'][\n",
    "    count_pivot['event_duration_pivot_count_磨蹭'] > 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每日磨蹭次数折线图'\n",
    "\n",
    "count_pivot['event_duration_pivot_count_磨蹭'][\n",
    "    count_pivot['event_duration_pivot_count_磨蹭'] > 0\n",
    "].plot(figsize=(20, 10), title=title)\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Count', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} 每日磨蹭次数直方图'\n",
    "\n",
    "count_pivot['event_duration_pivot_count_磨蹭'][\n",
    "    count_pivot['event_duration_pivot_count_磨蹭'] > 0\n",
    "].plot(kind='hist', figsize=(20, 10), title=title)\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Count', fontsize=20)\n",
    "plt.ylabel('Frequency', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.8) 每年, 月, 星期的事项关键词词云图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '2.4.8'\n",
    "# 年度关键词词云图\n",
    "col = 'year'\n",
    "year_key_words = part2.get_key_words(df, col)\n",
    "part2.plot_key_word_cloud(year_key_words, col, part, path=path, mask=1)\n",
    "year_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年度关键词词云图\n",
    "part2.plot_key_word_cloud(year_key_words, col, part, path=path, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022年每月关键词\n",
    "col = 'month'\n",
    "month_key_words = part2.get_key_words(df[df['year'] == '2022'], col)\n",
    "part2.plot_key_word_cloud(month_key_words, col, part, path=path)\n",
    "month_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022年 每day_period关键词\n",
    "col = 'day_period'\n",
    "day_period_key_words = part2.get_key_words(df[df['year'] == '2022'], col)\n",
    "part2.plot_key_word_cloud(day_period_key_words, col, part, path=path)\n",
    "day_period_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022年 duration_attr关键词\n",
    "col = 'duration_attr'\n",
    "duration_attr_key_words = part2.get_key_words(df[df['year'] == '2022'], col)\n",
    "part2.plot_key_word_cloud(duration_attr_key_words, col, part, path=path)\n",
    "duration_attr_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022年 attr关键词\n",
    "col = 'attr'\n",
    "attr_key_words = part2.get_key_words(df[df['year'] == '2022'], col, threshold=.8)\n",
    "part2.plot_key_word_cloud(attr_key_words, col, part, path=path)\n",
    "attr_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022年 weekday关键词\n",
    "col = 'weekday'\n",
    "weekday_key_words = part2.get_key_words(df[df['year'] == '2022'], col, threshold=1)\n",
    "part2.plot_key_word_cloud(weekday_key_words, col, part, path=path)\n",
    "weekday_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022年 mid_hour关键词\n",
    "col = 'mid_hour'\n",
    "midhour_key_words = part2.get_key_words(df[df['year'] == '2022'], col, threshold=1)\n",
    "part2.plot_key_word_cloud(midhour_key_words, col, part, path=path)\n",
    "midhour_key_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.9) 每天事项的对应先后以及月度关联的网络图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.8) 每天事项的对应先后以及月度关联的网络图\n",
    "part = '2.4.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.month2event(df, 2019, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.month2event(df, 2020, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.month2event(df, 2021, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.month2event(df, 2022, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.PostEvent(df, 2019, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.PostEvent(df, 2020, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.PostEvent(df, 2021, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2.PostEvent(df, 2022, event2attr, part, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 一键式EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AV = AutoViz_Class()\n",
    "dft = AV.AutoViz(\n",
    "    filename=None,  \n",
    "    sep=\",\",  \n",
    "    depVar=\"duration\",  \n",
    "    dfte=df,  \n",
    "    header=0,\n",
    "    verbose=0,  \n",
    "    lowess=True,  \n",
    "    chart_format=\"svg\",  \n",
    "    max_rows_analyzed=150000,  \n",
    "    max_cols_analyzed=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 衍生数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AV = AutoViz_Class()\n",
    "dft = AV.AutoViz(\n",
    "    filename=None,  \n",
    "    sep=\",\",  \n",
    "    depVar=\"effective time\",  \n",
    "    dfte=daily_attr_copy,  \n",
    "    header=0,\n",
    "    verbose=0,  \n",
    "    lowess=True,  \n",
    "    chart_format=\"svg\",  \n",
    "    max_rows_analyzed=150000,  \n",
    "    max_cols_analyzed=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 方差分析每月或是每年的事项是否有差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 原始事项时长数据的方差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始事项数据的方差分析\n",
    "part = '3.1.1'\n",
    "\n",
    "def vr_event(data, num, cate):\n",
    "    formula = f'{num}~C({cate})'\n",
    "    anova_re_2019 = anova_lm(ols(formula, data=data[[num, cate]]).fit())\n",
    "    return anova_re_2019.loc[f'C({cate})', 'PR(>F)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事项时长数据\n",
    "vr_duration_table = pd.DataFrame()\n",
    "for event in tqdm(df['event'].unique()):\n",
    "    for cate in cate_vars:\n",
    "        if cate not in ['event', 'attr']:\n",
    "            temp = df[df['event'] == event].copy()\n",
    "            temp[cate] = temp[cate].astype(str)\n",
    "            res = vr_event(temp, 'duration', cate)\n",
    "            vr_duration_table.loc[event, cate] = res\n",
    "\n",
    "# 该表格是各个事项的时长数据关于各分类变量的方差分析结果\n",
    "vr_duration_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取那些在不同分类情况下时长有显著差异的数据\n",
    "vr_duration_table.to_excel('事项时长数据的方差分析结果.xlsx', index=True)\n",
    "significant_event = vr_duration_table[vr_duration_table > .05].dropna(axis=0, how='all')\n",
    "significant_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于有显著差异的事项 做出在不同分类中均值的bar图\n",
    "mins = 30  # 剔除掉出现次数少的事项\n",
    "for e in tqdm(significant_event.index):\n",
    "    if len(df[df['event'] == e]) >= mins:\n",
    "        for c in significant_event.columns:\n",
    "            if significant_event.loc[e, c] >= .05:\n",
    "                plt.figure(dpi=100)\n",
    "                df[df['event'] == e].groupby(c)['duration'].agg('mean').plot(\n",
    "                    kind='bar', figsize=(20, 10)\n",
    "                )\n",
    "                title = f'{part} mean value of {e} duration in different {c} groups'\n",
    "                plt.title(title, fontsize=20)\n",
    "                plt.xlabel(e, fontsize=20)\n",
    "                plt.ylabel('Mean value', fontsize=20)\n",
    "                plt.xticks(fontsize=20)\n",
    "                plt.yticks(fontsize=20)\n",
    "                file = os.path.join(path, f'{title}.png')\n",
    "                plt.savefig(file, dpi=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只分析2022年\n",
    "vr_duration_table2022 = pd.DataFrame()\n",
    "for event in tqdm(df['event'].unique()):\n",
    "    for cate in cate_vars:\n",
    "        if cate not in ['event', 'attr', 'year']:\n",
    "            try:\n",
    "                temp = df[\n",
    "                    (df['event'] == event) & (df['year'] == '2022')\n",
    "                ].copy()\n",
    "                temp[cate] = temp[cate].astype(str)\n",
    "                res = vr_event(temp, 'duration', cate)\n",
    "                vr_duration_table2022.loc[event, cate] = res\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# 该表格是各个事项的时长数据关于各分类变量的方差分析结果\n",
    "vr_duration_table2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_event2022 = vr_duration_table2022[vr_duration_table2022 > .05].\\\n",
    "    dropna(axis=0, how='all')\n",
    "\n",
    "mins = 20\n",
    "\n",
    "for e in tqdm(significant_event2022.index):\n",
    "    if len(df[df['event'] == e]) >= mins:\n",
    "        for c in significant_event2022.columns:\n",
    "            if significant_event2022.loc[e, c] >= .05:\n",
    "                plt.figure(dpi=100)\n",
    "                df[\n",
    "                    (df['event'] == e) & (df['year'] == '2022')\n",
    "                ].groupby(c)['duration'].agg('mean').plot(\n",
    "                    kind='bar', figsize=(20, 10)\n",
    "                )\n",
    "                title = f'{part} mean value of {e} duration in different {c} groups in year 2022'\n",
    "                plt.title(title, fontsize=20)\n",
    "                plt.xlabel(e, fontsize=20)\n",
    "                plt.ylabel('Mean value', fontsize=20)\n",
    "                plt.xticks(fontsize=20)\n",
    "                plt.yticks(fontsize=20)\n",
    "                file = os.path.join(path, f'{title}.png')\n",
    "                plt.savefig(file, dpi=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 衍生数据的方差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.1.2'\n",
    "\n",
    "daily_attr['weekday'] = pd.to_datetime(daily_attr['date']).dt.weekday + 1\n",
    "daily_attr['year'] = daily_attr['date'].apply(lambda x:str(x).split('-')[0])\n",
    "daily_attr['month'] = daily_attr['date'].apply(lambda x:str(x).split('-')[1])\n",
    "daily_attr['day'] = daily_attr['date'].apply(lambda x:str(x).split('-')[2])\n",
    "daily_attr['week_order'] = daily_attr['date'].map(lambda x: pd.Period(x, freq='W'))\n",
    "daily_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变量类型划分\n",
    "num_vars = [\n",
    "    '有效时间', '浪费时间', '必要时间', '手机时间'\n",
    "]\n",
    "\n",
    "cates_vars = [\n",
    "    'year', 'month', 'day', 'weekday', 'week_order'\n",
    "]\n",
    "\n",
    "# 逐一进行方差分析\n",
    "daily_sum_table = pd.DataFrame()\n",
    "for num in tqdm(num_vars):\n",
    "    for cate in cates_vars:\n",
    "        temp = daily_attr[[num, cate]].copy()\n",
    "        temp[cate] = temp[cate].astype(str)\n",
    "        res = vr_event(temp, num, cate)\n",
    "        daily_sum_table.loc[num, cate] = res\n",
    "\n",
    "# 该表格是各个事项的时长日度求和数据数据关于各分类变量的方差分析结果\n",
    "daily_sum_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于有显著差异的事项 做出在不同分类中均值的bar图\n",
    "for e in tqdm(daily_sum_table.index):\n",
    "    for c in daily_sum_table.columns:\n",
    "        if daily_sum_table.loc[e, c] >= .05:\n",
    "            plt.figure(dpi=100)\n",
    "            daily_attr.groupby(c)[e].agg('mean').plot(\n",
    "                kind='bar', figsize=(20, 10)\n",
    "            )\n",
    "            title = f'{part} mean value of {e} duration in different {c} groups'\n",
    "            plt.title(title, fontsize=20)\n",
    "            plt.xlabel(e, fontsize=20)\n",
    "            plt.ylabel('Mean value', fontsize=20)\n",
    "            plt.xticks(fontsize=20)\n",
    "            plt.yticks(fontsize=20)\n",
    "            file = os.path.join(path, f'{title}.png')\n",
    "            plt.savefig(file, dpi=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 LSTM预测前后事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 建立字符索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars = df['event'].values\n",
    "idx_to_char = list(set(corpus_chars)) # 去重，得到索引到字符的映射\n",
    "char_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射\n",
    "vocab_size = len(char_to_idx)\n",
    "print(vocab_size)\n",
    "print('len(corpus_chars) = ', len(corpus_chars))\n",
    "\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列\n",
    "sample = corpus_indices[: 20]\n",
    "print('chars:', ' '.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 建立LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 128, vocab_size\n",
    "pred_period, pred_len, prefixes = 50, 50, ['磨蹭', '运动']\n",
    "rnn_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "num_steps, batch_size = 35, 2\n",
    "X = torch.rand(num_steps, batch_size, vocab_size)\n",
    "state = None\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "print(Y, state_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = part3.RNNModel(rnn_layer, vocab_size).to(device)\n",
    "part3.predict_rnn_pytorch(['运动'], 10, model, vocab_size, device, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "model = part3.RNNModel(rnn_layer, vocab_size).to(device)\n",
    "num_epochs, batch_size, lr, clipping_theta = 100, 64, 1e-2, 1e-2\n",
    "print_num = 10\n",
    "pred_period = num_epochs / print_num\n",
    "pred_len, prefixes = 15, [\n",
    "    ['华尔街日报'], \n",
    "    ['晚饭'], \n",
    "    ['摄影'], \n",
    "    ['接单工作'], \n",
    "    ['gis'],\n",
    "    ['量化'], \n",
    "    ['玩游戏'],\n",
    "]\n",
    "part3.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "    corpus_indices, idx_to_char, char_to_idx,\n",
    "    num_epochs, num_steps, lr, clipping_theta,\n",
    "    batch_size, pred_period, pred_len, prefixes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)\n",
    "model = part3.RNNModel(rnn_layer, vocab_size).to(device)\n",
    "num_epochs, batch_size, lr, clipping_theta = 100, 64, 1e-2, 1e-2\n",
    "print_num = 10\n",
    "pred_period = num_epochs / print_num\n",
    "pred_len, prefixes = 15, [\n",
    "    ['华尔街日报'], \n",
    "    ['晚饭'], \n",
    "    ['接单工作'], \n",
    "    ['托福听力'], \n",
    "    ['磨蹭'],\n",
    "    ['写日记']\n",
    "]\n",
    "part3.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n",
    "    corpus_indices, idx_to_char, char_to_idx,\n",
    "    num_epochs, num_steps, lr, clipping_theta,\n",
    "    batch_size, pred_period, pred_len, prefixes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 时长数据的简单聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.3'\n",
    "\n",
    "dates = pd.DataFrame(\n",
    "    pd.date_range(\n",
    "    start='2022-01-01',\n",
    "    end='2022-12-31'\n",
    ").astype(str), columns=['date']\n",
    ")\n",
    "\n",
    "data = pd.merge(\n",
    "    dates, daily_attr[num_vars + ['date']], on='date', how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergings = linkage(data.drop('date', axis=1).dropna())\n",
    "plt.figure(figsize=(20, 10))\n",
    "dendrogram(\n",
    "    mergings,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10, \n",
    ")\n",
    "plt.yticks(fontsize=20)\n",
    "title = f'{part} 2022时长数据层次聚类图'\n",
    "plt.title(title, fontsize=10)\n",
    "plt.savefig(os.path.join(path, f'{title}.png'), dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 关于日期的缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对日度数据做处理 只分析2022年\n",
    "part = '3.4'\n",
    "\n",
    "# 生成新特征\n",
    "data['weekday'] = pd.to_datetime(data['date']).dt.weekday + 1\n",
    "data['month'] = data['date'].apply(lambda x:str(x).split('-')[1])\n",
    "data['day'] = data['date'].apply(lambda x:str(x).split('-')[2])\n",
    "data['week_order'] = data['date'].map(lambda x: pd.Period(x, freq='W'))\n",
    "\n",
    "data.set_index('date')[num_vars].plot(\n",
    "    figsize=(20, 10)\n",
    ")\n",
    "\n",
    "title = '各类时长每日总和的折线图'\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Sum of duration', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.legend(loc=1, fontsize=20)\n",
    "\n",
    "print('缺失的行数有', data['有效时间'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样三次样条处理缺失\n",
    "for col in data.columns:\n",
    "    if '时间' in col:\n",
    "        upper, lower = data[col].max(), data[col].min()\n",
    "        data[col] = data[col].interpolate(method='spline', order=3)\n",
    "        # 避免插值结果太超过想象\n",
    "        data[col] = np.where(abs(data[col]) > upper, upper, data[col])\n",
    "        data[col] = np.where(data[col] < lower, lower, data[col])\n",
    "data = data.fillna(axis=0, method='bfill') \n",
    "\n",
    "\n",
    "title = f'{part} 处理缺失后的每日时长总和图'\n",
    "data.set_index('date')[num_vars].plot(\n",
    "    figsize=(20, 10), title=title\n",
    ")\n",
    "\n",
    "plt.title(title, fontsize=20)\n",
    "plt.xlabel('Date', fontsize=20)\n",
    "plt.ylabel('Sum of duration', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.legend(loc=1, fontsize=20)\n",
    "\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100) \n",
    "\n",
    "print('缺失的行数有', data['有效时间'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 关于日期的傅里叶分解求周期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有效时间\n",
    "part3.FRT(data, '有效时间', part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 浪费时间\n",
    "part3.FRT(data, '浪费时间', part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要时间\n",
    "part3.FRT(data, '必要时间', part, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手机时间\n",
    "part3.FRT(data, '手机时间', part, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 关于日期的传统时间序列分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 ARIMA分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.6.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4类时间的ARIMA分析\n",
    "arima_dict = {}\n",
    "for col in num_vars:\n",
    "    try:\n",
    "        arima_dict[col] = part3.ARIMA_auto(data.set_index('date')[col], col, part, path)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 GARCH分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.6.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检验是否存在GARCH效应\n",
    "arch_p_dict = {\n",
    "    k: v.arch_p for k, v in arima_dict.items() \n",
    "}\n",
    "print(arch_p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对存在GARCH效应的序列做GARCH建模\n",
    "for name, p in arch_p_dict.items():\n",
    "    if p < 0.05:\n",
    "        try:\n",
    "            part3.garch_auto(\n",
    "                data.set_index('date')[name], \n",
    "                name, \n",
    "                arima_dict[name].arima_order,\n",
    "                part,\n",
    "                path\n",
    "            ) \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3 VAR分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.6.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_vol_int_var = part3.VAR(data[list(arima_dict.keys())], arima_dict, part, path)\n",
    "price_vol_int_var.impulse_responses()\n",
    "price_vol_int_var.variance_decompositon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 关于日期的NeuralProphet 时间序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.7'\n",
    "NP_dict = {}\n",
    "test_length = 30\n",
    "\n",
    "for col in num_vars:\n",
    "    part3.MyNeuralProphet(data, col, NP_dict, test_length, part, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 关联规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '3.8'\n",
    "te = TransactionEncoder()\n",
    "df_tf = te.fit_transform(df.groupby('date')['event'].agg(list))\n",
    "dff = pd.DataFrame(df_tf,columns=te.columns_)\n",
    "# use_colnames=True表示使用元素名字，默认的False使用列名代表元素, 设置最小支持度min_support\n",
    "frequent_itemsets = apriori(dff, min_support=0.05, use_colnames=True)\n",
    "frequent_itemsets.sort_values(by='support', ascending=False, inplace=True)\n",
    "# 选择2频繁项集\n",
    "nwdf = frequent_itemsets[frequent_itemsets.itemsets.apply(lambda x: len(x)) == 2]\n",
    "nwdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwdf['from'] = nwdf['itemsets'].map(lambda x:list(x)[0])\n",
    "nwdf['to'] = nwdf['itemsets'].map(lambda x:list(x)[1])\n",
    "nwdf['weight'] = nwdf['support']\n",
    "part2.net(nwdf, 'from', 'to', '2022', event2attr, part, path, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric可以有很多的度量选项，返回的表列名都可以作为参数\n",
    "association_rule = association_rules(frequent_itemsets,metric='confidence',min_threshold=0.9)\n",
    "\n",
    "#关联规则可以提升度排序\n",
    "association_rule.sort_values(by='lift',ascending=False,inplace=True)\n",
    "association_rule.to_csv('association_rule.csv', index=False, encoding='gbk')\n",
    "association_rule\n",
    "# 规则是：antecedents->consequents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.数据挖掘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 通过生成聚合特征做日度时长数据的特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [\n",
    "    '浪费时间', '必要时间', '手机时间'\n",
    "]\n",
    "\n",
    "cates = [\n",
    "    'month', 'day', 'weekday'\n",
    "]\n",
    "\n",
    "y = '有效时间'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于A/B的统计特征\n",
    "for num_var in tqdm(nums):\n",
    "    for cate_var in cates:\n",
    "        for sts in ['mean', 'median', 'std', 'max', 'min']:\n",
    "            data[f'{cate_var}_{num_var}_{sts}'] = data.groupby(cate_var)[num_var].\\\n",
    "                transform(sts).values\n",
    "        data[f'{cate_var}_{num_var}_cv'] = data[f'{cate_var}_{num_var}_std'] / \\\n",
    "            data[f'{cate_var}_{num_var}_mean']\n",
    "        for fun in funcs.all_functions:\n",
    "            try:\n",
    "                fun_name = fun.__name__\n",
    "            except:\n",
    "                fun_name = str(fun)\n",
    "            data[f'{num_var}_{fun_name}_{cate_var}'] = data.groupby(cate_var)[num_var].\\\n",
    "                transform(fun).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流量平滑特征\n",
    "# 基于A/B的统计特征\n",
    "for num_var in tqdm(nums):\n",
    "    for cate_var in cates:\n",
    "        data[f'{num_var}_div_{cate_var}_{num_var}_mean'] = data[num_var] / \\\n",
    "            (data[f'{cate_var}_{num_var}_mean'] + 1e-5)\n",
    "        data[f'{num_var}_div_{cate_var}_{num_var}_median'] = data[num_var] / \\\n",
    "            (data[f'{cate_var}_{num_var}_median'] + 1e-5)\n",
    "        # 黄金组合特征\n",
    "        data[f'{num_var}_minus_{cate_var}_{num_var}_mean'] = data[num_var] - \\\n",
    "            data[f'{cate_var}_{num_var}_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相邻有序类别统计特征的变化\n",
    "sts_list = ['max', 'min', 'mean', 'std', 'median']\n",
    "for num_var in tqdm(nums):\n",
    "    for cate_var in cates:\n",
    "        for sts in sts_list:\n",
    "            data[f'{cate_var}_{num_var}_{sts}_diff']  = data[f'{cate_var}_{num_var}_{sts}'] - \\\n",
    "                data.groupby(cate_var)[f'{cate_var}_{num_var}_{sts}'].shift()        \n",
    "            data[f'{cate_var}_{num_var}_{sts}_ratio'] = data[f'{cate_var}_{num_var}_{sts}'] / \\\n",
    "                data.groupby(cate_var)[f'{cate_var}_{num_var}_{sts}'].shift() \n",
    "            data[f'{cate_var}_{num_var}_{sts}_diff'].fillna(method='bfill', inplace=True)\n",
    "            data[f'{cate_var}_{num_var}_{sts}_ratio'].fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事项序列的tf-idf特征\n",
    "vec = TfidfVectorizer(ngram_range=(1, 3), min_df=.01)\n",
    "vec_features = vec.fit_transform(\n",
    "    df.groupby('date')['event'].agg(lambda x: ' '.join(x))\n",
    ")\n",
    "vec_features = pd.DataFrame(\n",
    "    vec_features.toarray(), \n",
    "    columns=vec.get_feature_names()\n",
    ")\n",
    "data = pd.concat(\n",
    "    [data, vec_features], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 类别变量的one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate_var in cates:\n",
    "    onehot = pd.get_dummies(data[cate_var], drop_first=True, prefix=f'{cate_var}_')\n",
    "    data = pd.concat([data.drop(cate_var, axis=1), onehot], axis=1)\n",
    "\n",
    "data.to_excel('data_FE.xlsx', index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 利用boruta筛选特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "data = pd.read_excel('data_FE.xlsx')\n",
    "data = data[data['date'] < '2022-12-31'].reset_index()\n",
    "\n",
    "# 处理缺失\n",
    "for col in data.columns:\n",
    "    if data[col].isnull().any():\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "data = data.dropna(axis=1, how='all')\n",
    "\n",
    "train_size = .8\n",
    "rows = int(data.shape[0] * train_size)\n",
    "train, test = data.iloc[:(rows + 1), :], data.iloc[(rows + 1):, :]\n",
    "X = train.drop(['date', 'week_order', y], axis=1)\n",
    "Y = train[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBRegressor()\n",
    "boruta = BorutaPy(\n",
    "   estimator = clf,\n",
    "   n_estimators = 'auto',\n",
    "   max_iter = 100 # number of trials to perform\n",
    ")\n",
    "\n",
    "# 模型训练\n",
    "boruta.fit(np.array(X), np.array(Y))\n",
    "# 输出结果\n",
    "green_area = X.columns[boruta.support_].to_list()\n",
    "blue_area  = X.columns[boruta.support_weak_].to_list()\n",
    "print('features in the green area:', green_area)\n",
    "print('features in the blue area:', blue_area)\n",
    "cols = green_area + blue_area\n",
    "train = train[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 利用PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data[cols]\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.loc[train.index, cols]# dt.loc[train.index, :]\n",
    "X_test = data.loc[test.index, cols]#dt.loc[test.index, :]\n",
    "y_train = data.loc[train.index, y]#data.loc[train.index, y]\n",
    "y_test = data.loc[test.index, y]#data.loc[test.index, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 利用optuna优化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=10) # for reproducibility\n",
    "\n",
    "def objective(trial):\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "    param = {\n",
    "        'objective': 'regression_l2',\n",
    "        'metric': 'mse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 700, 3000),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    return mean_squared_error(y_test, gbm.predict(X_test))\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "lgb_params = study.best_params\n",
    "print(lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'metric': 'mse',\n",
    "        'verbosity': 0,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 700, 3000),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "\n",
    "    gbm = xgb.train(param, dtrain)\n",
    "    return mean_squared_error(y_test, gbm.predict(xgb.DMatrix(X_test, label=np.ones_like(y_test))))\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "xgb_params = study.best_params\n",
    "print(xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 stacking 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = '4.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 sklearn中的stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "models = []\n",
    "mses = []\n",
    "\n",
    "gkf = part4.PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=14)\n",
    "splits = list(gkf.split(X_train, groups=X_train.index))\n",
    "for train_indices, test_indices in splits:\n",
    "        model = StackingRegressor(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestRegressor()),\n",
    "                ('ada', AdaBoostRegressor()),\n",
    "                ('gb', GradientBoostingRegressor()),\n",
    "                ('lgb', LGBMRegressor(**lgb_params)),\n",
    "                ('xgb', XGBRegressor(**xgb_params))\n",
    "            ],\n",
    "            final_estimator=LGBMRegressor(**lgb_params),\n",
    "            cv=2\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train\n",
    "        )\n",
    "        models.append(model)\n",
    "        mses.append(mean_squared_error(\n",
    "            model.predict(X_test), y_test\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mses, np.mean(mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame([model.predict(X_test) for model in models]).mean()\n",
    "title = f'{part} Predicted value VS real value use sklearn'\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, pred, s=600)\n",
    "plt.xlabel('real value', fontsize=20)\n",
    "plt.ylabel('predicted value', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(title, fontsize=20)\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 自己实现stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from utils import part4\n",
    "importlib.reload(part4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [part4.rf_reg, part4.ada_reg, part4.gb_reg, part4.xgb_reg, part4.lgb_reg]\n",
    "train_y = y_train.values\n",
    "pred = part4.stacking_pred(\n",
    "    X_train.values,\n",
    "    train_y, \n",
    "    X_test.values, \n",
    "    gkf,\n",
    "    clf_list, \n",
    "    label_split=range(len(train_y)),\n",
    "    clf_fin='lgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f'{part} Predicted value VS real value'\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, pred.reshape(1, -1)[0], s=600)\n",
    "plt.xlabel('real value', fontsize=20)\n",
    "plt.ylabel('predicted value', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.title(title, fontsize=20)\n",
    "file = os.path.join(path, f'{title}.png')\n",
    "plt.savefig(file, dpi=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.concat(\n",
    "    [y_test.reset_index(), \n",
    "    pd.Series(pred.reshape(1, -1)[0], name='预测有效时间').reset_index()], \n",
    "    axis=1\n",
    ")[['有效时间', '预测有效时间']]\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更多学习笔记，请关注公众号【杰然不同之GR】 #\n",
    "![公众号二维码](http://m.qpic.cn/psc?/V10twqic2oh0r6/TmEUgtj9EK6.7V8ajmQrEG6xI.X7icgy*l8zd9O9qB3X6.AQyIe0uOSHtI7ti9nULpRDinQuLz61UqAz2Qxai8Ay.JgGp*bQUKKMa4c9AyU!/b&bo=rgGuAa4BrgEBGT4!&rf=viewer_4&t=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5a98bc0709bb551cb0b40a68bfcb118c11ed773779c4b4ca5eb3852e4a8f5446"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
